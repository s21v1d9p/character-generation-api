# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Backend API for modular, scalable AI character image and video generation using LoRA-based character system. The system allows creating new characters by providing ~20 reference images, with automated LoRA training and registration.

**Stack**: RunPod (GPU), ComfyUI, SDXL (base model), Stable Video Diffusion (video), S3/Supabase (storage)

## Architecture

```
src/
â”œâ”€â”€ api/routes/          # FastAPI route handlers
â”‚   â”œâ”€â”€ characters.py    # Character CRUD + LoRA registration
â”‚   â”œâ”€â”€ generation.py    # /generate-image, /generate-video endpoints
â”‚   â””â”€â”€ health.py        # Health checks
â”œâ”€â”€ core/                # Configuration, dependencies, middleware
â”‚   â”œâ”€â”€ config.py        # Settings from environment
â”‚   â””â”€â”€ dependencies.py  # FastAPI dependency injection
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ comfyui/         # ComfyUI websocket client and workflow execution
â”‚   â”œâ”€â”€ training/        # LoRA training pipeline (kohya_ss or similar)
â”‚   â””â”€â”€ storage/         # Cloud storage (S3/Supabase) for generated assets
â”œâ”€â”€ models/              # SQLAlchemy/database models
â””â”€â”€ schemas/             # Pydantic request/response schemas

workflows/               # ComfyUI workflow JSON templates (single workflow serves all characters)
scripts/                 # Utility scripts for training, setup
```

## Key Design Decisions

- **Dynamic LoRA Loading**: LoRAs are parameter-driven, drop-in assets loaded by `character_id`
- **Single Workflow Pattern**: Same generation workflow serves ALL characters (no per-character workflows)
- **Async Generation**: Long-running tasks (training, generation) use background workers
- **Workflow Abstraction**: ComfyUI complexity hidden behind simple API interface
- **Cloud Storage**: Generated assets uploaded to S3/Supabase, organized by character
- **Multi-Pod Ready**: Architecture supports scaling to multiple GPU pods

## Commands

```bash
# Setup
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"

# Run API server (development)
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000

# Run tests
pytest                           # all tests
pytest tests/unit                # unit tests only
pytest tests/integration         # integration tests
pytest -k "test_name"            # single test by name

# Linting/formatting
ruff check src tests             # lint
ruff format src tests            # format

# Type checking
mypy src
```

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/characters` | GET | List all characters |
| `/characters` | POST | Create character (triggers LoRA training) |
| `/characters/{id}` | GET | Get character details |
| `/generate-image` | POST | Generate image with character |
| `/generate-video` | POST | Generate video with character |

## Environment Variables

See `.env.example` for required configuration:
- `RUNPOD_API_KEY`: RunPod API key for GPU pod management
- `COMFYUI_URL`: ComfyUI server websocket URL
- `STORAGE_PROVIDER`: `s3` or `supabase`
- `STORAGE_*`: Cloud storage credentials and bucket
- `LORA_OUTPUT_DIR`: Directory for trained LoRA files
- `DATABASE_URL`: PostgreSQL connection string

## GPU Pipeline (RunPod)

- ComfyUI runs on RunPod GPU pods
- SDXL base model for image generation
- Stable Video Diffusion for image-to-video
- LoRAs loaded dynamically per request (no workflow changes needed)
- Results saved to cloud storage automatically

## Generation Flow

1. Client calls `/generate-image` with `character_id` and prompt
2. API loads character metadata, resolves LoRA file path
3. ComfyUI workflow template populated with parameters
4. Workflow executed via ComfyUI API
5. Output image uploaded to cloud storage
6. Cloud URL returned to client

## Training Flow

1. Client calls `/characters` POST with name + ~20 reference images
2. Images preprocessed (captioning, resizing)
3. LoRA training job queued on GPU
4. On completion: `.safetensors` saved, character registered in DB
5. Character immediately available for generation (no workflow modifications)

## Scalability Requirements

- Adding new LoRAs requires NO changes to workflows
- LoRAs are drop-in, parameter-driven assets
- Same generation workflow serves all characters
- Architecture must support multiple GPU pods
- New models/workflows can be plugged in without major rewrites


<claude-mem-context>
# Recent Activity

<!-- This section is auto-generated by claude-mem. Edit content outside the tags. -->

### Jan 19, 2026

| ID | Time | T | Title | Read |
|----|------|---|-------|------|
| #1136 | 5:25 PM | âœ… | Environment Configuration File Created with RunPod Pod Details | ~483 |
| #1117 | 4:51 PM | ðŸ”µ | Git Ignore Configuration Excludes Models and Generated Assets | ~378 |
| #1112 | 4:50 PM | ðŸ”µ | Client Requirements for Automated LoRA Character System | ~455 |
| #1108 | 4:49 PM | ðŸ”µ | Docker Compose Development Stack with API, PostgreSQL, and Redis | ~398 |
| #1098 | 4:46 PM | ðŸ”µ | Environment Configuration for Multi-Service Architecture | ~390 |
| #1096 | " | ðŸ”µ | API Workflow for Character Creation and Generation | ~344 |
| #1095 | " | ðŸ”µ | Python Project Dependencies and Tooling Configuration | ~309 |
</claude-mem-context>